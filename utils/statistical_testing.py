"""
Bootstrap Statistical Testing for Process Insight Modeler
çµ±è¨ˆçš„æ¤œå®šï¼ˆBootstrapæ³•ï¼‰

æ—¢å­˜ã®åˆ†æçµæœã«ä¿¡é ¼åŒºé–“ã¨æœ‰æ„æ€§æ¤œå®šã‚’ä»˜ä¸ã—ã€çµ±è¨ˆçš„ä¿¡é ¼æ€§ã‚’æ‹…ä¿ã™ã‚‹ã€‚
"""

from typing import List, Dict, Tuple, Any, Callable
import numpy as np
import pandas as pd
import networkx as nx
import logging
from dataclasses import dataclass
import time

logger = logging.getLogger(__name__)


@dataclass
class BootstrapResult:
    """Bootstrapçµ±è¨ˆæ¤œå®šã®çµæœ"""
    metric_name: str  # "PageRank", "Shapley", "TE"ç­‰
    node_ci: Dict[str, Tuple[float, float, float]]  # ãƒãƒ¼ãƒ‰å â†’ (å€¤, ä¸‹é™, ä¸Šé™)
    group_comparison: pd.DataFrame  # ã‚°ãƒ«ãƒ¼ãƒ—é–“æ¯”è¼ƒï¼ˆpå€¤ä»˜ãï¼‰
    stable_findings: List[str]  # çµ±è¨ˆçš„ã«å®‰å®šã—ãŸçŸ¥è¦‹
    unstable_findings: List[str]  # ä¸å®‰å®šï¼ˆå†è©•ä¾¡æ¨å¥¨ï¼‰
    interpretation: str  # å¹³æ˜“ãªè§£é‡ˆæ–‡
    computation_time: float
    n_bootstrap: int  # ãƒªã‚µãƒ³ãƒ—ãƒ«å›æ•°
    alpha: float  # æœ‰æ„æ°´æº–


class BootstrapTester:
    """
    Bootstrapçµ±è¨ˆæ¤œå®šã‚¯ãƒ©ã‚¹
    
    ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ³•ã§ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ã—ã€Permutationæ¤œå®šã§æœ‰æ„æ€§ã‚’è©•ä¾¡ã€‚
    """
    
    def __init__(
        self,
        adjacency_matrix: np.ndarray,
        node_names: List[str],
        node_groups: Dict[str, str] = None,
        n_bootstrap: int = 1000,
        alpha: float = 0.05
    ):
        """
        Args:
            adjacency_matrix: éš£æ¥è¡Œåˆ—ï¼ˆNÃ—Nï¼‰
            node_names: ãƒãƒ¼ãƒ‰åãƒªã‚¹ãƒˆ
            node_groups: ãƒãƒ¼ãƒ‰å â†’ ã‚°ãƒ«ãƒ¼ãƒ—åã®è¾æ›¸ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            n_bootstrap: ãƒªã‚µãƒ³ãƒ—ãƒ«å›æ•°
            alpha: æœ‰æ„æ°´æº–ï¼ˆ0.05 = 95%ä¿¡é ¼åŒºé–“ï¼‰
        """
        self.matrix = adjacency_matrix.copy()
        self.node_names = node_names
        self.node_groups = node_groups or {}
        self.n = len(node_names)
        self.n_bootstrap = n_bootstrap
        self.alpha = alpha
        
        logger.info(f"BootstrapTesteråˆæœŸåŒ–: {self.n}ãƒãƒ¼ãƒ‰, {n_bootstrap}ãƒªã‚µãƒ³ãƒ—ãƒ«")
    
    def bootstrap_confidence_interval(
        self,
        metric_func: Callable[[np.ndarray], Dict[str, float]],
        progress_callback: Callable[[int, int], None] = None
    ) -> Dict[str, Tuple[float, float, float]]:
        """
        Bootstrapæ³•ã§ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—
        
        Args:
            metric_func: éš£æ¥è¡Œåˆ—ã‚’å—ã‘å–ã‚Šã€{ãƒãƒ¼ãƒ‰å: ã‚¹ã‚³ã‚¢}ã‚’è¿”ã™é–¢æ•°
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯(current, total)
        
        Returns:
            {ãƒãƒ¼ãƒ‰å: (å…ƒã®å€¤, ä¸‹é™, ä¸Šé™)}
        """
        # å…ƒã®å€¤ã‚’è¨ˆç®—
        original_scores = metric_func(self.matrix)
        
        # éã‚¼ãƒ­è¦ç´ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        nonzero_i, nonzero_j = np.where(self.matrix != 0)
        n_edges = len(nonzero_i)
        
        if n_edges == 0:
            logger.warning("éã‚¼ãƒ­è¦ç´ ãŒã‚ã‚Šã¾ã›ã‚“")
            return {node: (0.0, 0.0, 0.0) for node in self.node_names}
        
        # Bootstrap ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        bootstrap_samples = []
        
        for b in range(self.n_bootstrap):
            # å¾©å…ƒæŠ½å‡ºã§ãƒªã‚µãƒ³ãƒ—ãƒ«
            resampled_indices = np.random.choice(n_edges, size=n_edges, replace=True)
            
            # ãƒªã‚µãƒ³ãƒ—ãƒ«è¡Œåˆ—ã‚’æ§‹ç¯‰
            resampled_matrix = np.zeros_like(self.matrix)
            
            for idx in resampled_indices:
                i, j = nonzero_i[idx], nonzero_j[idx]
                resampled_matrix[i, j] += self.matrix[i, j]
            
            # æ­£è¦åŒ–ï¼ˆæœŸå¾…å€¤ã‚’å…ƒã¨åŒã˜ã«ï¼‰
            resampled_matrix = resampled_matrix / n_edges * n_edges
            
            try:
                # ãƒ¡ãƒˆãƒªãƒƒã‚¯è¨ˆç®—
                resampled_scores = metric_func(resampled_matrix)
                bootstrap_samples.append(resampled_scores)
            except Exception as e:
                logger.warning(f"Bootstrapã‚µãƒ³ãƒ—ãƒ«{b}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
            
            if progress_callback and (b + 1) % 50 == 0:
                progress_callback(b + 1, self.n_bootstrap)
        
        # ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—
        ci_results = {}
        
        for node_name in original_scores:
            # ã“ã®ãƒãƒ¼ãƒ‰ã®Bootstrapã‚µãƒ³ãƒ—ãƒ«
            node_samples = [
                sample.get(node_name, 0) for sample in bootstrap_samples
            ]
            
            if len(node_samples) == 0:
                ci_results[node_name] = (original_scores[node_name], 0, 0)
                continue
            
            # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«æ³•
            lower = np.percentile(node_samples, self.alpha/2 * 100)
            upper = np.percentile(node_samples, (1 - self.alpha/2) * 100)
            
            ci_results[node_name] = (
                original_scores[node_name],
                lower,
                upper
            )
        
        return ci_results
    
    def permutation_test(
        self,
        metric_func: Callable,
        group_a_nodes: List[str],
        group_b_nodes: List[str],
        n_permutations: int = 1000
    ) -> Dict[str, Any]:
        """
        2ç¾¤é–“ã®å·®ã®Permutationæ¤œå®š
        
        Args:
            metric_func: ãƒ¡ãƒˆãƒªãƒƒã‚¯è¨ˆç®—é–¢æ•°
            group_a_nodes: ã‚°ãƒ«ãƒ¼ãƒ—Aã®ãƒãƒ¼ãƒ‰åãƒªã‚¹ãƒˆ
            group_b_nodes: ã‚°ãƒ«ãƒ¼ãƒ—Bã®ãƒãƒ¼ãƒ‰åãƒªã‚¹ãƒˆ
            n_permutations: Permutationå›æ•°
        
        Returns:
            æ¤œå®šçµæœã®è¾æ›¸
        """
        # å…ƒã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        original_scores = metric_func(self.matrix)
        
        # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®å¹³å‡å€¤
        group_a_scores = [original_scores.get(node, 0) for node in group_a_nodes]
        group_b_scores = [original_scores.get(node, 0) for node in group_b_nodes]
        
        if len(group_a_scores) == 0 or len(group_b_scores) == 0:
            return {
                "observed_diff": 0,
                "p_value": 1.0,
                "significant": False,
                "null_distribution": []
            }
        
        observed_diff = np.mean(group_a_scores) - np.mean(group_b_scores)
        
        # Nullåˆ†å¸ƒã‚’ç”Ÿæˆ
        null_distribution = []
        
        pooled_nodes = group_a_nodes + group_b_nodes
        n_a = len(group_a_nodes)
        
        for _ in range(n_permutations):
            # ãƒ©ãƒ™ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å…¥ã‚Œæ›¿ãˆ
            perm_indices = np.random.permutation(len(pooled_nodes))
            perm_nodes = [pooled_nodes[i] for i in perm_indices]
            
            perm_group_a = perm_nodes[:n_a]
            perm_group_b = perm_nodes[n_a:]
            
            perm_a_scores = [original_scores.get(node, 0) for node in perm_group_a]
            perm_b_scores = [original_scores.get(node, 0) for node in perm_group_b]
            
            perm_diff = np.mean(perm_a_scores) - np.mean(perm_b_scores)
            null_distribution.append(perm_diff)
        
        # på€¤è¨ˆç®—ï¼ˆä¸¡å´æ¤œå®šï¼‰
        p_value = (np.abs(null_distribution) >= np.abs(observed_diff)).mean()
        
        return {
            "observed_diff": observed_diff,
            "p_value": p_value,
            "significant": p_value < self.alpha,
            "null_distribution": null_distribution,
            "group_a_mean": np.mean(group_a_scores),
            "group_b_mean": np.mean(group_b_scores)
        }
    
    def run_comprehensive_bootstrap_analysis(
        self,
        metric_name: str = "PageRank",
        metric_func: Callable = None,
        progress_callback: Callable[[str, float], None] = None
    ) -> BootstrapResult:
        """
        åŒ…æ‹¬çš„ãªBootstrapåˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            metric_name: ãƒ¡ãƒˆãƒªãƒƒã‚¯å
            metric_func: ãƒ¡ãƒˆãƒªãƒƒã‚¯è¨ˆç®—é–¢æ•°ï¼ˆNoneã®å ´åˆã¯PageRankï¼‰
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯(message, progress_pct)
        
        Returns:
            BootstrapResult
        """
        start_time = time.time()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯PageRank
        if metric_func is None:
            def pagerank_func(matrix):
                G = nx.from_numpy_array(matrix, create_using=nx.DiGraph)
                try:
                    pr = nx.pagerank(G, weight='weight')
                except:
                    pr = nx.pagerank(G)
                return pr
            
            metric_func = pagerank_func
        
        # 1. ä¿¡é ¼åŒºé–“è¨ˆç®—
        if progress_callback:
            progress_callback("Bootstrapä¿¡é ¼åŒºé–“è¨ˆç®—ä¸­...", 0.0)
        
        def bootstrap_progress(current, total):
            if progress_callback:
                pct = 0.0 + 0.7 * (current / total)
                progress_callback(f"Bootstrap {current}/{total}...", pct)
        
        ci_results = self.bootstrap_confidence_interval(
            metric_func,
            progress_callback=bootstrap_progress
        )
        
        # 2. å®‰å®šæ€§è©•ä¾¡
        stable_findings = []
        unstable_findings = []
        
        for node, (value, lower, upper) in ci_results.items():
            # ç›¸å¯¾èª¤å·®
            if abs(value) > 1e-6:
                rel_error = (upper - lower) / (2 * abs(value))
                
                if rel_error < 0.2:  # 20%ä»¥å†…
                    stable_findings.append(
                        f"{node}: {value:.4f} [{lower:.4f}, {upper:.4f}]"
                    )
                else:
                    unstable_findings.append(
                        f"{node}: {value:.4f} [{lower:.4f}, {upper:.4f}] (ç›¸å¯¾èª¤å·®{rel_error*100:.1f}%)"
                    )
        
        # 3. ã‚°ãƒ«ãƒ¼ãƒ—é–“æ¯”è¼ƒ
        if progress_callback:
            progress_callback("ã‚°ãƒ«ãƒ¼ãƒ—é–“æ¯”è¼ƒå®Ÿè¡Œä¸­...", 0.7)
        
        group_comparison = self._compare_groups(metric_func)
        
        computation_time = time.time() - start_time
        logger.info(f"Bootstrapåˆ†æå®Œäº†: {computation_time:.2f}ç§’")
        
        # 4. è§£é‡ˆæ–‡ç”Ÿæˆ
        interpretation = self._generate_interpretation(
            metric_name, ci_results, stable_findings, unstable_findings, group_comparison
        )
        
        return BootstrapResult(
            metric_name=metric_name,
            node_ci=ci_results,
            group_comparison=group_comparison,
            stable_findings=stable_findings,
            unstable_findings=unstable_findings,
            interpretation=interpretation,
            computation_time=computation_time,
            n_bootstrap=self.n_bootstrap,
            alpha=self.alpha
        )
    
    def _compare_groups(self, metric_func: Callable) -> pd.DataFrame:
        """ã‚°ãƒ«ãƒ¼ãƒ—é–“æ¯”è¼ƒï¼ˆPermutationæ¤œå®šï¼‰"""
        
        if not self.node_groups:
            return pd.DataFrame()
        
        group_names = list(set(self.node_groups.values()))
        
        if len(group_names) < 2:
            return pd.DataFrame()
        
        comparison_results = []
        
        for i, group_a in enumerate(group_names):
            for group_b in group_names[i+1:]:
                nodes_a = [n for n, g in self.node_groups.items() if g == group_a]
                nodes_b = [n for n, g in self.node_groups.items() if g == group_b]
                
                if len(nodes_a) == 0 or len(nodes_b) == 0:
                    continue
                
                perm_result = self.permutation_test(
                    metric_func, nodes_a, nodes_b, n_permutations=500
                )
                
                comparison_results.append({
                    "ã‚°ãƒ«ãƒ¼ãƒ—A": group_a,
                    "ã‚°ãƒ«ãƒ¼ãƒ—B": group_b,
                    "å¹³å‡å€¤A": perm_result["group_a_mean"],
                    "å¹³å‡å€¤B": perm_result["group_b_mean"],
                    "å¹³å‡å€¤ã®å·®": perm_result["observed_diff"],
                    "på€¤": perm_result["p_value"],
                    "æœ‰æ„æ€§": "âœ… æœ‰æ„" if perm_result["significant"] else "âŒ éæœ‰æ„"
                })
        
        if len(comparison_results) == 0:
            return pd.DataFrame()
        
        df = pd.DataFrame(comparison_results)
        df = df.sort_values(by="på€¤")
        
        return df
    
    def _generate_interpretation(
        self,
        metric_name: str,
        ci_results: Dict,
        stable_findings: List[str],
        unstable_findings: List[str],
        group_comparison: pd.DataFrame
    ) -> str:
        """å¹³æ˜“ãªæ—¥æœ¬èªã®è§£é‡ˆæ–‡ã‚’ç”Ÿæˆ"""
        
        n_stable = len(stable_findings)
        n_unstable = len(unstable_findings)
        total = len(ci_results)
        
        interpretation = f"""
## ğŸ“Š Bootstrapçµ±è¨ˆæ¤œå®šçµæœã®è§£é‡ˆ

### {metric_name}ã®ä¿¡é ¼æ€§è©•ä¾¡

**å®‰å®šæ€§ã‚µãƒãƒªãƒ¼:**
- çµ±è¨ˆçš„ã«å®‰å®š: {n_stable}/{total}ãƒãƒ¼ãƒ‰ ({n_stable/total*100:.1f}%)
- ä¸å®‰å®šï¼ˆå†è©•ä¾¡æ¨å¥¨ï¼‰: {n_unstable}/{total}ãƒãƒ¼ãƒ‰ ({n_unstable/total*100:.1f}%)
- ãƒªã‚µãƒ³ãƒ—ãƒ«å›æ•°: {self.n_bootstrap}
- ä¿¡é ¼æ°´æº–: {(1-self.alpha)*100:.0f}%

### âœ… å®‰å®šã—ãŸçŸ¥è¦‹ï¼ˆä¿¡é ¼ã§ãã‚‹ï¼‰
ä»¥ä¸‹ã®{metric_name}å€¤ã¯çµ±è¨ˆçš„ã«å®‰å®šã—ã¦ãŠã‚Šã€ä¿¡é ¼ã§ãã¾ã™ï¼ˆç›¸å¯¾èª¤å·®<20%ï¼‰:

"""
        
        # ä¸Šä½5ä»¶ã®å®‰å®šã—ãŸçŸ¥è¦‹
        for finding in stable_findings[:5]:
            interpretation += f"- {finding}\n"
        
        if len(stable_findings) > 5:
            interpretation += f"... ä»–{len(stable_findings) - 5}ä»¶\n"
        
        if len(unstable_findings) > 0:
            interpretation += f"""
### âš ï¸ ä¸å®‰å®šãªçŸ¥è¦‹ï¼ˆå†è©•ä¾¡æ¨å¥¨ï¼‰
ä»¥ä¸‹ã®ãƒãƒ¼ãƒ‰ã¯ä¿¡é ¼åŒºé–“ãŒåºƒãã€å†è©•ä¾¡ãŒæ¨å¥¨ã•ã‚Œã¾ã™:

"""
            for finding in unstable_findings[:3]:
                interpretation += f"- {finding}\n"
        
        # ã‚°ãƒ«ãƒ¼ãƒ—é–“æ¯”è¼ƒ
        if len(group_comparison) > 0:
            interpretation += """
### ã‚°ãƒ«ãƒ¼ãƒ—é–“æ¯”è¼ƒï¼ˆPermutationæ¤œå®šï¼‰

"""
            significant_comparisons = group_comparison[
                group_comparison['æœ‰æ„æ€§'] == 'âœ… æœ‰æ„'
            ]
            
            if len(significant_comparisons) > 0:
                interpretation += f"ä»¥ä¸‹ã®ã‚°ãƒ«ãƒ¼ãƒ—é–“ã«ã¯çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ãŒã‚ã‚Šã¾ã™ (p<{self.alpha}):\n\n"
                for _, row in significant_comparisons.iterrows():
                    interpretation += (
                        f"- **{row['ã‚°ãƒ«ãƒ¼ãƒ—A']}** vs **{row['ã‚°ãƒ«ãƒ¼ãƒ—B']}**: "
                        f"å·®={row['å¹³å‡å€¤ã®å·®']:.4f}, p={row['på€¤']:.4f}\n"
                    )
            else:
                interpretation += "ã‚°ãƒ«ãƒ¼ãƒ—é–“ã«çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n"
        
        interpretation += """
### ğŸ’¡ æ´»ç”¨æ–¹æ³•

1. **ä¿¡é ¼ã§ãã‚‹çŸ¥è¦‹**: å®‰å®šã—ãŸä¸Šä½ãƒãƒ¼ãƒ‰ã‚’é‡ç‚¹ç®¡ç†å¯¾è±¡ã¨ã™ã‚‹
2. **å†è©•ä¾¡ç®‡æ‰€**: ä¸å®‰å®šãªãƒãƒ¼ãƒ‰ã¯è¿½åŠ ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
3. **ã‚°ãƒ«ãƒ¼ãƒ—æˆ¦ç•¥**: æœ‰æ„å·®ãŒã‚ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨
4. **å ±å‘Šè³‡æ–™**: ä¿¡é ¼åŒºé–“ä»˜ãã§çµŒå–¶å±¤ã«èª¬å¾—åŠ›ã‚ã‚‹èª¬æ˜ãŒå¯èƒ½

### ğŸ“– ä¿¡é ¼åŒºé–“ã®è¦‹æ–¹

- **ç‹­ã„åŒºé–“**: ãƒ‡ãƒ¼ã‚¿ãŒå®‰å®šã—ã¦ã„ã‚‹ã€ä¿¡é ¼æ€§ãŒé«˜ã„
- **åºƒã„åŒºé–“**: ãƒ‡ãƒ¼ã‚¿ã®ã°ã‚‰ã¤ããŒå¤§ãã„ã€è¿½åŠ è©•ä¾¡ãŒå¿…è¦
- **ã‚¼ãƒ­ã‚’å«ã‚€**: çµ±è¨ˆçš„ã«æœ‰æ„ã§ãªã„å¯èƒ½æ€§
"""
        
        return interpretation.strip()


def compute_stability_score(ci_results: Dict[str, Tuple[float, float, float]]) -> pd.DataFrame:
    """
    å®‰å®šæ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    
    Args:
        ci_results: {ãƒãƒ¼ãƒ‰å: (å€¤, ä¸‹é™, ä¸Šé™)}
    
    Returns:
        å®‰å®šæ€§ã‚¹ã‚³ã‚¢ã®DataFrame
    """
    stability_data = []
    
    for node, (value, lower, upper) in ci_results.items():
        if abs(value) > 1e-6:
            rel_error = (upper - lower) / (2 * abs(value))
            stability_score = 1 / (1 + rel_error)  # 0-1ã‚¹ã‚±ãƒ¼ãƒ«ã€é«˜ã„ã»ã©å®‰å®š
            
            stability_data.append({
                "ãƒãƒ¼ãƒ‰å": node,
                "å€¤": value,
                "ä¸‹é™": lower,
                "ä¸Šé™": upper,
                "ç›¸å¯¾èª¤å·®": rel_error,
                "å®‰å®šæ€§ã‚¹ã‚³ã‚¢": stability_score,
                "åˆ¤å®š": "âœ… å®‰å®š" if rel_error < 0.2 else "âš ï¸ ã‚„ã‚„ä¸å®‰å®š" if rel_error < 0.5 else "âŒ ä¸å®‰å®š"
            })
    
    df = pd.DataFrame(stability_data)
    df = df.sort_values(by="å®‰å®šæ€§ã‚¹ã‚³ã‚¢", ascending=False)
    
    return df
